{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tda4Yz3rG2v7"
      },
      "source": [
        "# **Automated Machine Learning**\n",
        "\n",
        "---\n",
        "\n",
        "### **Torch Reptile - Parallel Metalearning**\n",
        "*Fall 2020 | Ruduan B.F. Plug*\n",
        "\n",
        "---\n",
        "\n",
        "<font size=\"1\">*Based on the Original Implementation by Alex Nichol & John Schulman [[1]](https://openai.com/blog/reptile/)*</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzQ1T6HWX-S5"
      },
      "source": [
        "### Meta Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YuqzrXatX_j1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]\n"
          ]
        }
      ],
      "source": [
        "# System Utility\n",
        "import sys\n",
        "\n",
        "# IPython Notebook Utilities\n",
        "from IPython.display import clear_output\n",
        "import tqdm.notebook as tqdm\n",
        "clear_output()\n",
        "\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHOC5HMhP3sA"
      },
      "source": [
        "### Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MZ0KiCd6P34h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch Version\t 2.2.0+cu121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gbena/.local/lib/python3.11/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m device \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch Version\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Device\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/cuda/__init__.py:423\u001b[0m, in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    412\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mname\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/cuda/__init__.py:453\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 453\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/cuda/__init__.py:302\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    301\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 302\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    306\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW"
          ]
        }
      ],
      "source": [
        "# Data Processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Model Library\n",
        "# import tensorflow as tf\n",
        "\n",
        "# Parallel Compute\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Utility Libraries\n",
        "import random\n",
        "import math\n",
        "from time import time\n",
        "from copy import deepcopy\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize Device\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Torch Version\\t\", torch.__version__)\n",
        "print(\"Using Device\\t\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahVLMwR55Rsz"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WM48Nv385dcL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f968833a770>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_folder = \"data\"\n",
        "np.random.seed(int(time()))\n",
        "torch.manual_seed(int(time()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFHo5IKxtA_L"
      },
      "source": [
        "### Reptile TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xti4UH95t4TI"
      },
      "source": [
        "#### Class Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2TEFbIOEtIR2"
      },
      "outputs": [],
      "source": [
        "class Reptile:\n",
        "\n",
        "  def __init__(self, model, log, params):\n",
        "\n",
        "    # Intialize Reptile Parameters\n",
        "    self.inner_step_size = params[0]\n",
        "    self.inner_batch_size = params[1]\n",
        "    self.outer_step_size = params[2]\n",
        "    self.outer_iterations = params[3]\n",
        "    self.meta_batch_size = params[4] \n",
        "    self.eval_iterations = params[5] \n",
        "    self.eval_batch_size = params[6]\n",
        "\n",
        "    # Initialize Torch Model and Tensorboard\n",
        "    self.model = model.to(device)\n",
        "    self.log = log\n",
        "\n",
        "  def reset(self):\n",
        "\n",
        "    # Reset Training Gradients\n",
        "    self.model.zero_grad()\n",
        "    self.current_loss = 0\n",
        "    self.current_batch = 0\n",
        "\n",
        "  def train(self, task):\n",
        "\n",
        "    # Train from Scratch\n",
        "    self.reset()\n",
        "\n",
        "    # Outer Training Loop\n",
        "    for outer_iteration in tqdm.tqdm(range(self.outer_iterations)):\n",
        "\n",
        "      # Track Current Weights\n",
        "      current_weights = deepcopy(self.model.state_dict())\n",
        "\n",
        "      # Sample a new Subtask\n",
        "      samples, task_theta = sample(task)\n",
        "\n",
        "      # Inner Training Loop\n",
        "      for inner_iteration in range(self.inner_batch_size):\n",
        "\n",
        "        # Process Meta Learning Batches\n",
        "        for batch in range(0, len(sample_space), self.meta_batch_size):\n",
        "\n",
        "          # Get Permuted Batch from Sample\n",
        "          perm = np.random.permutation(len(sample_space))\n",
        "          idx = perm[batch: batch + self.meta_batch_size][:, None]\n",
        "\n",
        "          # Calculate Batch Loss\n",
        "          batch_loss = self.loss(sample_space[idx], samples[idx])\n",
        "          batch_loss.backward()\n",
        "\n",
        "          # Update Model Parameters\n",
        "          for theta in self.model.parameters():\n",
        "\n",
        "            # Get Parameter Gradient\n",
        "            grad = theta.grad.data\n",
        "\n",
        "            # Update Model Parameter\n",
        "            theta.data -= self.inner_step_size * grad\n",
        "\n",
        "          # Update Model Loss from Torch Model Tensor\n",
        "          loss_tensor = batch_loss.cpu()\n",
        "          self.current_loss += loss_tensor.data.numpy()\n",
        "          self.current_batch += 1\n",
        "\n",
        "      # Linear Cooling Schedule\n",
        "      alpha = self.outer_step_size * (1 - outer_iteration / self.outer_iterations)\n",
        "\n",
        "      # Get Current Candidate Weights\n",
        "      candidate_weights = self.model.state_dict()\n",
        "\n",
        "      # Transfer Candidate Weights to Model State Checkpoint\n",
        "      state_dict = {candidate: (current_weights[candidate] + alpha * \n",
        "                               (candidate_weights[candidate] - current_weights[candidate])) \n",
        "                                for candidate in candidate_weights}\n",
        "      self.model.load_state_dict(state_dict)\n",
        "      \n",
        "      # Log new Training Loss\n",
        "      self.log.add_scalars('Model Estimate/Loss', \n",
        "                           {'Loss' : self.current_loss / self.current_batch}, \n",
        "                           outer_iteration)\n",
        "\n",
        "  def loss(self, x, y):\n",
        "\n",
        "    # Reset Torch Gradient\n",
        "    self.model.zero_grad()\n",
        "\n",
        "    # Calculate Torch Tensors\n",
        "    x = torch.tensor(x, device = device, dtype = torch.float32)\n",
        "    y = torch.tensor(y, device = device, dtype = torch.float32)\n",
        "\n",
        "    # Estimate over Sample\n",
        "    yhat = self.model(x)\n",
        "\n",
        "    # Regression Loss over Estimate\n",
        "    loss = nn.MSELoss()\n",
        "    output = loss(yhat, y)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def predict(self, x):\n",
        "\n",
        "    # Estimate using Torch Model\n",
        "    t = torch.tensor(x, device = device, dtype = torch.float32)\n",
        "    t = self.model(t)\n",
        "\n",
        "    # Bring Torch Tensor from GPU to System Host Memory\n",
        "    t = t.cpu()\n",
        "\n",
        "    # Return Estimate as Numpy Float\n",
        "    y = t.data.numpy()\n",
        "\n",
        "    return y\n",
        "\n",
        "  def eval(self, base_truth, meta_batch_size, gradient_steps, inner_step_size):\n",
        "\n",
        "    # Sample Points from Task Sample Space\n",
        "    x, y = sample_points(base_truth, self.meta_batch_size)\n",
        "\n",
        "    # Model Base Estimate over Sample Space\n",
        "    estimate = [self.predict(sample_space[:,None])]\n",
        "\n",
        "    # Store Meta-Initialization Weights\n",
        "    meta_weights = deepcopy(self.model.state_dict())\n",
        "\n",
        "    # Get Estimate Loss over Meta-Initialization\n",
        "    loss_t = self.loss(x,y).cpu()\n",
        "    meta_loss = loss_t.data.numpy()\n",
        "\n",
        "    # Calculcate Estimate over Gradient Steps\n",
        "    for step in range(gradient_steps):\n",
        "\n",
        "      # Calculate Evaluation Loss and Backpropagate\n",
        "      eval_loss = self.loss(x,y)\n",
        "      eval_loss.backward()\n",
        "\n",
        "      # Update Model Estimate Parameters\n",
        "      for theta in self.model.parameters():\n",
        "\n",
        "        # Get Parameter Gradient\n",
        "        grad = theta.grad.data\n",
        "\n",
        "        # Update Model Parameter\n",
        "        theta.data -= self.inner_step_size * grad\n",
        "\n",
        "      # Update Estimate over Sample Space\n",
        "      estimate.append(self.predict(sample_space[:, None]))\n",
        "\n",
        "    # Get Estimate Loss over Evaluation\n",
        "    loss_t = self.loss(x,y).cpu()\n",
        "    estimate_loss = loss_t.data.numpy()\n",
        "    evaluation_loss = abs(meta_loss - estimate_loss)/meta_batch_size\n",
        "    \n",
        "    # Restore Meta-Initialization Weights\n",
        "    self.model.load_state_dict(meta_weights)\n",
        "\n",
        "    return estimate, evaluation_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARn0cnaCT4I6"
      },
      "source": [
        "#### PyTorch Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fzQaKtGLT4YP"
      },
      "outputs": [],
      "source": [
        "class TorchModule(nn.Module):\n",
        "\n",
        "  def __init__(self, n):\n",
        "\n",
        "    # Initialize PyTorch Base Module\n",
        "    super(TorchModule, self).__init__()\n",
        "\n",
        "    # Define Multi-Layer Perceptron\n",
        "    self.input = nn.Linear(1,n)\n",
        "    self.hidden_in = nn.Linear(n,n)\n",
        "    self.hidden_out = nn.Linear(n,n)\n",
        "    self.output = nn.Linear(n,1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # PyTorch Feed Forward Subroutine\n",
        "    x = torch.tanh(self.input(x))\n",
        "    x = torch.tanh(self.hidden_in(x))\n",
        "    x = torch.tanh(self.hidden_out(x))\n",
        "    y = self.output(x)\n",
        "\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdfjr05yiF_T"
      },
      "source": [
        "### Learning Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2O25f8RUHmr"
      },
      "source": [
        "#### Task Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yNUMR1Z3iGUi"
      },
      "outputs": [],
      "source": [
        "def logistic(x, theta):\n",
        "\n",
        "  return theta[0] / (1 + np.exp(-1 * theta[1] * ( x - theta[2])))\n",
        "\n",
        "def normal(x, theta):\n",
        "\n",
        "  return 1/(theta[1] * np.sqrt(2 * np.pi)) * np.exp(-1/2 * np.power((x-theta[0])/theta[1],2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAf2MjOoUKL8"
      },
      "source": [
        "#### Task Sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OhxxbAe1ULpM"
      },
      "outputs": [],
      "source": [
        "def sample(task):\n",
        "\n",
        "  if task is (not normal or not logistic):\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "  # Parametric Generator for Task Distribution\n",
        "  theta = [np.random.uniform( 1,  5), \n",
        "           np.random.uniform( 0, 10),\n",
        "           np.random.uniform( 0, 10)]\n",
        "\n",
        "  return task(sample_space, theta), theta\n",
        "\n",
        "def sample_points(task, batch_size):\n",
        "\n",
        "  # Sample Random Points from Sample Space\n",
        "  idx = np.random.choice(np.arange(len(sample_space)), batch_size, replace = False)\n",
        "  return sample_space[idx[:,None]], task[idx[:,None]]\n",
        "\n",
        "def meta_sample(radius, count):\n",
        "\n",
        "  # Generate Sample Space of Specified Radius\n",
        "  sample_space = np.linspace(-radius, radius, count)\n",
        "  return sample_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9W4j4rW0yuS"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RDm4ny3Qt0em"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55e091caaabf4f098c376fe09f5e574e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define Experiment Parameters\n",
        "inner_step_size = 0.02\n",
        "inner_batch_size = 16\n",
        "\n",
        "outer_step_size = 0.1\n",
        "outer_iterations = 10000\n",
        "meta_batch_size = 32\n",
        "\n",
        "eval_iterations = 32\n",
        "eval_batch_size = 10\n",
        "eval_range = range(1,11)\n",
        "\n",
        "model_size = 32\n",
        "sample_radius = 20\n",
        "sample_count = 100\n",
        "\n",
        "params = [inner_step_size, inner_batch_size,\n",
        "          outer_step_size, outer_iterations, meta_batch_size,\n",
        "          eval_iterations, eval_batch_size]\n",
        "\n",
        "# Define Experiment Task and Model\n",
        "task = logistic\n",
        "log = SummaryWriter(data_folder)\n",
        "model = Reptile(TorchModule(model_size), log, params)\n",
        "\n",
        "# Train Model\n",
        "eval_mse = np.empty(shape=[len(eval_range), eval_batch_size])\n",
        "sample_space = meta_sample(sample_radius, sample_count)\n",
        "model.train(task)\n",
        "\n",
        "# Evaluate Model\n",
        "for batch in range(eval_batch_size):\n",
        "\n",
        "  samples, task_theta  = sample(task)\n",
        "\n",
        "  for sample_size in eval_range:\n",
        "\n",
        "    # Estimate Model for Batch\n",
        "    estimate, loss = model.eval(samples, sample_size, eval_iterations, inner_step_size)\n",
        "    eval_mse[sample_size-1, batch-1] = loss\n",
        "    \n",
        "    # Log Results to Tensorboard\n",
        "    for idx in range(len(samples)):\n",
        "        log.add_scalars('Model Evaluation {}/{} Samples'.format(batch + 1, sample_size), \n",
        "            {'Task': samples[idx], \n",
        "              'Baseline': estimate[0][idx][0], \n",
        "              'Estimate' : estimate[-1][idx][0]}, \n",
        "              idx)\n",
        "\n",
        "log.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jiPJ9EM9278"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SixEsDGC0y7s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ERROR: Failed to launch TensorBoard (exited with 1).\n",
              "Contents of stderr:\n",
              "TensorFlow installation not found - running with reduced feature set.\n",
              "\n",
              "NOTE: Using experimental fast data loading logic. To disable, pass\n",
              "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
              "    https://github.com/tensorflow/tensorboard/issues/4784\n",
              "\n",
              "Address already in use\n",
              "Port 6006 is in use by another program. Either identify and stop that program, or start the server with a different port."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /content/data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Torch Reptile Repository",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
